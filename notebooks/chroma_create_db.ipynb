{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"data_dir\": r\"data\\pdfs\",\n",
    "        \"persist_directory\": \"chroma_langchain_db\",\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"output_document_from_vector_store\": 5,\n",
    "        \"chunk_size\": 1000,\n",
    "        \"chunk_overlap\": 10,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"embedding_model\": \"mxbai-embed-large\",\n",
    "        \"llm_model\": \"llama3.2:3b\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the Path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"__file__\").resolve().parent.parent.parent / config[\"data\"][\"data_dir\"]\n",
    "persist_directory = (\n",
    "    Path(\"__file__\").resolve().parent.parent.parent / config['data'][\"persist_directory\"]\n",
    ")\n",
    "config[\"data\"][\"data_dir\"] = data_dir\n",
    "config['data'][\"persist_directory\"] = persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name_mapping_dict = {\n",
    "    \"c0\": \"Alienware alpha or Alienware steam machine\",\n",
    "    \"c1\": \"XPS 27 7760\",\n",
    "    \"c2\": \"Alienware 13 R3\",\n",
    "    \"c3\": \"Dell Alienware m16 R1\",\n",
    "    \"c4\": \"Alienware m17 R4\",\n",
    "    \"c5\": \"Alienware x17 R2\",\n",
    "    \"c6\": \"Chromebook 11 3180\",\n",
    "    \"c7\": \"Dell G15 5510\",\n",
    "    \"c8\": \"ASUS ROG Strix SCAR 17 (2023)\",\n",
    "    \"c9\": \"ROG Zephyrus G16 (2024) GU605\",\n",
    "    \"c10\": \"Dell XPS 13 9370\",\n",
    "    \"c11\": \"Dell XPS 14 9440\",\n",
    "    \"c12\": \"Dell XPS 15 9500\",\n",
    "    \"c13\": \"Dell XPS 16 9640\",\n",
    "    \"c14\": \"XPS 17 9730\",\n",
    "    \"c15\": \"Dell Alienware m16 R2\",\n",
    "    \"c16\": \"Alienware x14 R2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OllamaEmbeddings(\n",
    "#     model=config['model'][\"embedding_model\"],\n",
    "# )\n",
    "\n",
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# # llm = ChatOllama(model=config['model'][\"llm_model\"], callbacks=callback_manager)\n",
    "# llm = ChatOllama(model=config['model'][\"llm_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RUN THE CELL BELOW Twice!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"Iterate over the data directory. Splits the pdf's and returns list of documents.\n",
    "    Args\n",
    "    ----\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    documents: list\n",
    "        List of splitted documents.\n",
    "    \"\"\"\n",
    "\n",
    "    documents: list = []\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "        chunk_size=config[\"train\"][\"chunk_size\"],\n",
    "\n",
    "        chunk_overlap=config[\"train\"][\"chunk_overlap\"],\n",
    "\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    class_abbreviation: list[str] = os.listdir(config['data'][\"data_dir\"])\n",
    "\n",
    "\n",
    "    for item in tqdm(class_abbreviation):\n",
    "\n",
    "        path_till_individual_folder: str = config['data'][\"data_dir\"] / item\n",
    "\n",
    "        for individual_pdf in os.listdir(path_till_individual_folder):\n",
    "\n",
    "            actual_name_pdf: str = class_name_mapping_dict[item].strip()\n",
    "\n",
    "            loader = PyPDFLoader(\n",
    "                os.path.join(path_till_individual_folder, individual_pdf)\n",
    "            )\n",
    "\n",
    "            temp_docs = loader.load()\n",
    "\n",
    "            splitted_docs = text_splitter.split_documents(temp_docs)\n",
    "\n",
    "            for doc in splitted_docs:\n",
    "                doc.metadata[\"category\"] = actual_name_pdf\n",
    "                doc.metadata.pop(\"source\")\n",
    "                doc.metadata.pop(\"page\")\n",
    "\n",
    "            documents.extend(splitted_docs)\n",
    "\n",
    "    logger.info(f\"The total length of the extracted pdf: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# documents = get_data()\n",
    "# uuids = [str(uuid4()) for _ in range(len(documents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initiate Vector Store\n",
    "# persistent_client = chromadb.PersistentClient(path=str(config['data'][\"persist_directory\"]))\n",
    "# collection = persistent_client.get_or_create_collection(\"InsightAICollection\")\n",
    "# vector_store_from_client = Chroma(\n",
    "#     client=persistent_client,\n",
    "#     collection_name=\"test_collection\",\n",
    "#     embedding_function=embeddings,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistent_client.heartbeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the below cell to add documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc, uuid in tqdm(zip(documents, uuids), total=len(documents), desc=\"Adding documents\"):\n",
    "#     vector_store_from_client.add_documents(documents=[doc], ids=[uuid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is the RAM of the model?\"\n",
    "# image_class = \"Alienware alpha or Alienware steam machine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved_docs = vector_store_from_client.similarity_search_with_score(\n",
    "#     query=query, k=5, filter={\"category\": image_class}\n",
    "# )\n",
    "# query_embeddings = embeddings.embed_query(query)\n",
    "# retrieved_docs_from_embeddings = vector_store_from_client.similarity_search_by_vector(\n",
    "#     query_embeddings, k=5, filter={\"category\": image_class}\n",
    "# )\n",
    "# print(retrieved_docs)\n",
    "# print(retrieved_docs_from_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model=config['model'][\"embedding_model\"],\n",
    ")\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = ChatOllama(model=config['model'][\"llm_model\"], callbacks=callback_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:chromadb.api.segment:Collection InsightAICollection already exists, returning existing collection.\n",
      "DEBUG:chromadb.api.segment:Collection test_collection already exists, returning existing collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://us.i.posthog.com:443 \"POST /batch/ HTTP/11\" 200 15\n"
     ]
    }
   ],
   "source": [
    "# Initiate Vector Store\n",
    "persistent_client = chromadb.PersistentClient(path=str(config['data'][\"persist_directory\"]))\n",
    "collection = persistent_client.get_or_create_collection(\"InsightAICollection\")\n",
    "vector_store_from_client = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"test_collection\",\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C1F4754FB0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Wed, 02 Oct 2024 10:11:42 GMT'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:chromadb.config:Starting component PersistentLocalHnswSegment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://us.i.posthog.com:443 \"POST /batch/ HTTP/11\" 200 15\n"
     ]
    }
   ],
   "source": [
    "query = \"Show me the list of specification of integrated Graphics Processing Unit (GPU) supported by the machine ?\"\n",
    "image_class = class_name_mapping_dict['c13']\n",
    "\n",
    "retriever = vector_store_from_client.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": config['train']['output_document_from_vector_store'], \"filter\": {\"category\": image_class}},\n",
    ")\n",
    "retrived_query = retriever.invoke(query)\n",
    "formatated_docs = \"\\n\\n\".join(doc.page_content for doc in retrived_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'category': 'Dell XPS 16 9640'}, page_content='Table 21. GPU—Integrated\\xa0\\nController Memory size Processor\\nIntel Arc Graphics Shared system memory ●Intel Core Ultra 7 155H\\n●Intel Core Ultra 7 165H\\n●Intel Core Ultra 9 185H\\nSpecifications of XPS 16 9640 25'), Document(metadata={'category': 'Dell XPS 16 9640'}, page_content='GPU—Discrete\\nThe following table lists the specifications of the discrete Graphics Processing Unit (GPU) supported by your XPS 16 9640 .\\nTable 22. GPU—Discrete\\xa0\\nController Memory size Memory type\\nNVIDIA GeForce RTX 4050 6 GB GDDR6\\nNVIDIA GeForce RTX 4060 8 GB GDDR6\\nNVIDIA GeForce RTX 4070 8 GB GDDR6\\nMultiple display support matrix\\nThe following table lists the multiple display support matrix for your XPS 16 9640 .\\nTable 23. Multiple display support matrix\\xa0\\nGraphics Card Discrete Graphics \\nController Direct Output \\nModeSupported external displays \\nwith computer internal display \\nonSupported external \\ndisplays with computer \\ninternal display off\\nNVIDIA GeForce RTX \\n4050 and 4060Not supported ●Three connected displays with \\nDisplayPort support up to 4K/ \\n60 Hz.\\n●One connected display with \\nDisplayPort supports up to 8K/ \\n60 Hz:\\n○One DisplayPort cable \\nsupports up to 8K/ 30 Hz, \\nand\\n○Two DisplayPort cables \\nsupport up to 8K/ 60Hz.●Four connected displays \\nwith DisplayPort support'), Document(metadata={'category': 'Dell XPS 16 9640'}, page_content='Operating system\\nYour XPS 16 9640  supports the following operating systems:\\n●Windows 11 Home, 64-bit\\n●Windows 11 Pro, 64-bit\\nMemory\\nThe following table lists the memory specifications of your XPS 16 9640 .\\nTable 5. Memory specifications\\xa0\\nDescription For computers shipped \\nwith Intel Arc GraphicsFor computers shipped \\nwith NVIDIA GeForce \\nRTX 4050 or NVIDIA \\nGeForce RTX 4060 \\nGPUs:For computers shipped \\nwith NVIDIA GeForce RTX \\n4070 GPUs:\\nMemory slots Onboard Onboard Onboard\\nMemory type LPDDR5x LPDDR5x LPDDR5x\\nMemory speed 6400 MT/s 6400 MT/s 7467 MT/s\\nMaximum memory configuration 16 GB 64 GB 64 GB\\nMinimum memory configuration 16 GB 16 GB 32 GB\\nMemory configurations supported 16 GB: LPDDR5x, 6400 \\nMT/s (onboard)●16 GB: LPDDR5x, 6400 \\nMT/s (onboard)\\n●32 GB: LPDDR5x, 6400 \\nMT/s (onboard)\\n●64 GB: LPDDR5x, 6400 \\nMT/s (onboard)●32 GB: LPDDR5x, 7467 \\nMT/s (onboard)\\n●64 GB: LPDDR5x, 7467 \\nMT/s (onboard)\\nExternal ports\\nThe following table lists the external ports of your XPS 16 9640 .'), Document(metadata={'category': 'Dell XPS 16 9640'}, page_content='Table 1. Display support (up to 4K/ 60 Hz)\\xa0\\nGraphics card Supported external displays with \\ncomputer internal display onSupported external displays with \\ncomputer internal display off\\nIntel Arc Graphics (Integrated \\ngraphics)Three Four\\nIntel Arc Graphics with NVIDIA \\nGeForce RTX 4050, 4060, or 4070Three Four\\nTable 2. Display support (up to 8K/ 60 Hz)\\xa0\\nGraphics card Supported external displays with \\ncomputer internal display onSupported external displays with \\ncomputer internal display off\\nIntel Arc Graphics (Integrated \\ngraphics)One One\\nIntel Arc Graphics with NVIDIA \\nGeForce RTX 4050, 4060, or 4070One One\\nThe following table shows the supported accessories for connecting external displays to your computer.\\nTable 3. Supported accessories for connecting external displays\\xa0\\nAccessories\\nDell Thunderbolt Dock - WD22TB4\\nDell Adapter - USB-C to HDMI/DisplayPort with Power Pass-Through\\nSupplied USB Type-C to USB Type A and DisplayPort dongle to connect to a DisplayPort device'), Document(metadata={'category': 'Dell XPS 16 9640'}, page_content='Heat sink .............................................................................................................................................................................. 53\\nRemoving the heat sink—Integrated GPUs ......................................................................................................... 53\\nInstalling the heat sink—Integrated GPU .............................................................................................................. 54\\nRemoving the heat sink—Discrete GPU ............................................................................................................... 55\\nInstalling the heat sink—Discrete GPU .................................................................................................................. 57\\nWireless-module shield .................................................................................................................................................... 58')]\n"
     ]
    }
   ],
   "source": [
    "print(retrived_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\"\"You are an assistant for question-answering tasks.\n",
    "#             Use the following pieces context to answer the question.\n",
    "#             If the context does not contain answer, just say that you don't know. Do not add anything on your own.\\n\\n\n",
    "#             Question: {query}\n",
    "#            Context:{formatated_docs}\n",
    "#            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C83D2466C0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Wed, 02 Oct 2024 09:10:32 GMT'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specifications of integrated Graphics Processing Unit (GPU) supported by the machine are as follows:\n",
      "\n",
      "1. Intel Arc Graphics:\n",
      "   - Controller: Shared system memory\n",
      "   - Memory size: Not specified (indicated as ●)\n",
      "\n",
      "Note that the table does not provide detailed specifications for the integrated GPU, only stating that it is a shared system memory with no specific details provided."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The specifications of integrated Graphics Processing Unit (GPU) supported by the machine are as follows:\\n\\n1. Intel Arc Graphics:\\n   - Controller: Shared system memory\\n   - Memory size: Not specified (indicated as ●)\\n\\nNote that the table does not provide detailed specifications for the integrated GPU, only stating that it is a shared system memory with no specific details provided.', response_metadata={'model': 'llama3.2:3b', 'created_at': '2024-10-02T09:10:39.8036998Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 40659520700, 'load_duration': 28128500, 'prompt_eval_count': 1031, 'prompt_eval_duration': 33658856000, 'eval_count': 75, 'eval_duration': 6970280000}, id='run-85d178fa-2958-4b2a-a698-06316857f5fd-0', usage_metadata={'input_tokens': 1031, 'output_tokens': 75, 'total_tokens': 1106})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(f\"\"\"You are an expert assistant for question answering tasks. Use the following context information to answer the question.\n",
    "            If the context does not contain answer, just say that you don't know.\\n\\n\n",
    "            Question: {query}\n",
    "            Context:{formatated_docs}\n",
    "           \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insightai-WytcrPNm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
